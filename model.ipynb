{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_weight(name, shape, dtype=tf.float32, relu=True):\n",
    "    if relu:\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype, \n",
    "                                initializer=tf.variance_scaling_initializer(scale=2.))\n",
    "    else:\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype,\n",
    "                                initializer=tf.variance_scaling_initializer(distribution='uniform'))\n",
    "\n",
    "\n",
    "def _get_bias(name, shape, dtype=tf.float32):\n",
    "    return tf.get_variable(name=name, shape=shape, dtype=dtype,\n",
    "                            initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_layer(inputs, dropout_keep_prob, n_convs, training):\n",
    "    \n",
    "    def _inception(name, inputs, in_channels, out_channels, training):\n",
    "        \n",
    "        def batch_norm(x):\n",
    "            mean, var = tf.nn.moments(x, axes=[0, 1, 2])\n",
    "            bn = tf.nn.batch_normalization(x, mean, var, 0, 1, 1e-3, name='batch_normal')\n",
    "            return bn\n",
    "\n",
    "        def _conv(name, inputs, in_channels, out_channels, ksize, training):\n",
    "            with tf.variable_scope(name):\n",
    "                if ksize == 1:\n",
    "                    kernel = _get_weight(name='kernel', shape=[ksize, ksize, in_channels, out_channels], relu=False)\n",
    "                    bias = _get_bias(name='bias', shape=[out_channels])\n",
    "                    conv = tf.nn.conv2d(inputs, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "                    conv_add_bias = tf.nn.bias_add(conv, bias)\n",
    "                    #bn = tf.layers.batch_normalization(conv_add_bias, name='batch_normal')\n",
    "                    #bn = batch_norm(conv_add_bias)\n",
    "                    bn = conv_add_bias\n",
    "                    #bn = tf.layers.batch_normalization(conv_add_bias, training=training, name='batch_normal')            \n",
    "                    relu = tf.nn.relu(bn, name='relu')\n",
    "                else:\n",
    "                    kernel = _get_weight(name='kernel1', shape=[1, ksize, in_channels, in_channels], relu=False)\n",
    "                    conv = tf.nn.conv2d(inputs, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "                    relu = tf.nn.relu(conv, name='relu1')\n",
    "                    #relu = conv\n",
    "                    \n",
    "                    kernel = _get_weight(name='kernel2', shape=[ksize, 1, in_channels, out_channels], relu=False)\n",
    "                    conv = tf.nn.conv2d(conv, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "                    relu = tf.nn.relu(conv, name='relu2')\n",
    "            return relu\n",
    "       \n",
    "        with tf.variable_scope(name):\n",
    "            k1_channels = out_channels / 2\n",
    "            conv1_1 = _conv('conv1_1', inputs, in_channels, k1_channels, ksize=1, training=training)\n",
    "            conv1_2 = _conv('conv1_2', conv1_1, k1_channels, k1_channels, ksize=3, training=training)\n",
    "            pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='max_pool1')\n",
    "\n",
    "            k1_channels = out_channels / 8 * 3\n",
    "            conv2_1 = _conv('conv2_1', inputs, in_channels, k1_channels, ksize=1, training=training)\n",
    "            conv2_2 = _conv('conv2_2', conv2_1, k1_channels, k1_channels, ksize=3, training=training)\n",
    "            conv2_3 = _conv('conv2_3', conv2_2, k1_channels, k1_channels, ksize=3, training=training)\n",
    "            pool2 = tf.nn.max_pool(conv2_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='max_pool2')\n",
    "\n",
    "            pool3 = tf.nn.max_pool(inputs, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='max_pool2')\n",
    "            conv3 = _conv('conv3', pool3, in_channels, out_channels/8, ksize=1, training=training)\n",
    "\n",
    "            concated = tf.concat([pool1, pool2, conv3], axis=3)\n",
    "            dropout = tf.nn.dropout(concated, dropout_keep_prob, name='dropout')\n",
    "        \n",
    "        return dropout\n",
    "\n",
    "    \n",
    "    for i in range(n_convs):\n",
    "        if i == 0:\n",
    "            conv = _inception('inception1', inputs, 3, 8, training)\n",
    "        else:\n",
    "            conv = _inception('inception%d' % (i+1), conv, 2**(i+2), 2**(i+3), training)\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rnn_layer(inputs, dropout_keep_prob):\n",
    "    with tf.variable_scope('bidirectional_rnn'):\n",
    "        col_wise = tf.transpose(inputs, [0, 2, 1, 3])\n",
    "        shape = col_wise.get_shape().as_list()\n",
    "        col_size = shape[2] * shape[3]\n",
    "        rnn_inputs = tf.reshape(col_wise, [-1, shape[1], col_size])\n",
    "        \n",
    "        num_units = int(col_size)\n",
    "        cell_unit = tf.nn.rnn_cell.GRUCell\n",
    "        cell_fw = cell_unit(num_units, name='cell_fw')\n",
    "        cell_fw_dropout = tf.nn.rnn_cell.DropoutWrapper(cell_fw, output_keep_prob=dropout_keep_prob)        \n",
    "        cell_bw = cell_unit(num_units, name='cell_bw')\n",
    "        cell_bw_dropout = tf.nn.rnn_cell.DropoutWrapper(cell_bw, output_keep_prob=dropout_keep_prob)\n",
    "        \n",
    "        outputs_tuple, final_state_tuple = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, rnn_inputs, dtype=tf.float32)\n",
    "        outputs = tf.concat(axis=2, values=outputs_tuple)\n",
    "    \n",
    "        return outputs[:, -1], final_state_tuple, (cell_fw, cell_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _outputs_layer(inputs, dropout_keep_prob, n_len, n_classes, n_denses):    \n",
    "    \n",
    "    def _dense(name, inputs, in_dim, out_dim):\n",
    "        with tf.variable_scope(name):\n",
    "            w = _get_weight(name='weight', shape=[in_dim, out_dim])\n",
    "            b = _get_bias(name='bias', shape=[out_dim])\n",
    "            dense = tf.matmul(inputs, w) + b\n",
    "            bn = tf.layers.batch_normalization(dense, name='batch_normal')\n",
    "            relu = tf.nn.relu(bn)\n",
    "        return relu\n",
    "    \n",
    "    with tf.variable_scope('output'):\n",
    "        dim = inputs.get_shape().as_list()[-1]\n",
    "        dense = inputs\n",
    "        for i in range(n_denses):\n",
    "            dense = _dense('dense%d' % (i+1), dense, dim, dim/2)\n",
    "            dim /= 2\n",
    "        dropout = tf.nn.dropout(dense, dropout_keep_prob, name='dropout')\n",
    "    \n",
    "        w = _get_weight(name='weight', shape=[dim, n_classes*n_len])\n",
    "        b = _get_bias(name='bias', shape=[n_classes*n_len])\n",
    "        final = tf.matmul(dropout, w) + b\n",
    "        logits = tf.reshape(final, [-1, n_len, n_classes])\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(logits, labels, training):\n",
    "    class _Eval(object):\n",
    "        pass\n",
    "    result = _Eval()\n",
    "\n",
    "    name = 'train_eval' if training else 'test_eval'\n",
    "    with tf.variable_scope(name):\n",
    "        losses = []\n",
    "        n_len = logits.get_shape().as_list()[1]\n",
    "        for i in range(n_len):\n",
    "            losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[:, i], labels=labels[:,i]))\n",
    "        result.loss = tf.reduce_mean(losses, name='mean_loss')\n",
    "        tf.summary.scalar('loss', result.loss)\n",
    "\n",
    "        if not training:\n",
    "            preds = tf.argmax(logits, 2, name='preds')\n",
    "            result.preds = preds\n",
    "\n",
    "            correct_preds = tf.reduce_all(tf.equal(preds, labels), 1)\n",
    "            result.acc = tf.reduce_mean(tf.cast(correct_preds, tf.float32), name='acc')\n",
    "            result.acc_ = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32), name='acc_')\n",
    "            tf.summary.scalar('accuracy', result.acc)\n",
    "            tf.summary.scalar('accuracy_', result.acc_)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, images, labels, n_len, n_classes, n_convs=4, n_denses=1, \n",
    "                 learning_rate=1 / 1024, batch_size=64, training=True):        \n",
    "        if training:\n",
    "            self.batch_size = batch_size\n",
    "            dropout_keep_prob = 4 / 5\n",
    "        else:\n",
    "            self.batch_size = 32\n",
    "            dropout_keep_prob = 1.\n",
    "        \n",
    "        conv = _conv_layer(inputs=images, dropout_keep_prob=dropout_keep_prob, n_convs=n_convs, training=training)\n",
    "        print('_conv_layer\\t', conv)\n",
    "        \n",
    "        rnn_outputs, self.final_state, self.cells = _rnn_layer(inputs=conv, \n",
    "                                                               dropout_keep_prob=dropout_keep_prob)\n",
    "        self.init_state = (self.cells[0].zero_state(self.batch_size, dtype=tf.float32),\n",
    "                           self.cells[1].zero_state(self.batch_size, dtype=tf.float32))\n",
    "        print('_rnn_layer\\t', rnn_outputs)\n",
    "\n",
    "        logits = _outputs_layer(inputs=rnn_outputs, dropout_keep_prob=dropout_keep_prob, \n",
    "                                n_len=n_len, n_classes=n_classes, n_denses=n_denses)\n",
    "        print('_output_layer\\t', logits, '\\n')\n",
    "        \n",
    "        self.eval = _eval(logits=logits, labels=labels, training=training)\n",
    "        \n",
    "        \n",
    "        if training:\n",
    "            opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            #update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            #with tf.control_dependencies(update_ops):\n",
    "            gradients, _ =  tf.clip_by_global_norm(tf.gradients(self.eval.loss, tf.trainable_variables()), 5)\n",
    "            self.train_op = opt.apply_gradients(zip(gradients, tf.trainable_variables()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
